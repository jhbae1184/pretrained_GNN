{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # 대화형 모드\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "# modified from https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# --------------------------------------------------------\n",
    "# Position embedding utils\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# relative position embedding\n",
    "# References: https://arxiv.org/abs/2009.13658\n",
    "# --------------------------------------------------------\n",
    "def get_2d_relative_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, grid_size*grid_size]\n",
    "    \"\"\"\n",
    "    pos_embed = get_2d_sincos_pos_embed(embed_dim, grid_size)\n",
    "    relative_pos = 2 * np.matmul(pos_embed, pos_embed.transpose()) / pos_embed.shape[1]\n",
    "    return relative_pos\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def pairwise_distance(x):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square + x_inner + x_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def part_pairwise_distance(x, start_idx=0, end_idx=1):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_part = x[:, start_idx:end_idx]\n",
    "        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n",
    "        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square_part + x_inner + x_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def xy_pairwise_distance(x, y):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        xy_inner = -2*torch.matmul(x, y.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        y_square = torch.sum(torch.mul(y, y), dim=-1, keepdim=True)\n",
    "        return x_square + xy_inner + y_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def dense_knn_matrix(x, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.transpose(2, 1).squeeze(-1)\n",
    "        batch_size, n_points, n_dims = x.shape\n",
    "        ### memory efficient implementation ###\n",
    "        n_part = 10000\n",
    "        if n_points > n_part:\n",
    "            nn_idx_list = []\n",
    "            groups = math.ceil(n_points / n_part)\n",
    "            for i in range(groups):\n",
    "                start_idx = n_part * i\n",
    "                end_idx = min(n_points, n_part * (i + 1))\n",
    "                dist = part_pairwise_distance(x.detach(), start_idx, end_idx)\n",
    "                if relative_pos is not None:\n",
    "                    dist += relative_pos[:, start_idx:end_idx]\n",
    "                _, nn_idx_part = torch.topk(-dist, k=k)\n",
    "                nn_idx_list += [nn_idx_part]\n",
    "            nn_idx = torch.cat(nn_idx_list, dim=1)\n",
    "        else:\n",
    "            dist = pairwise_distance(x.detach())\n",
    "            if relative_pos is not None:\n",
    "                dist += relative_pos\n",
    "            _, nn_idx = torch.topk(-dist, k=k) # b, n, k\n",
    "        ######\n",
    "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
    "    return torch.stack((nn_idx, center_idx), dim=0)\n",
    "\n",
    "\n",
    "def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.transpose(2, 1).squeeze(-1)\n",
    "        y = y.transpose(2, 1).squeeze(-1)\n",
    "        batch_size, n_points, n_dims = x.shape\n",
    "        dist = xy_pairwise_distance(x.detach(), y.detach())\n",
    "        if relative_pos is not None:\n",
    "            dist += relative_pos\n",
    "        _, nn_idx = torch.topk(-dist, k=k)\n",
    "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
    "    return torch.stack((nn_idx, center_idx), dim=0)\n",
    "\n",
    "\n",
    "class DenseDilated(nn.Module):\n",
    "    \"\"\"\n",
    "    Find dilated neighbor from neighbor list\n",
    "\n",
    "    edge_index: (2, batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilated, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        if self.stochastic:\n",
    "            if torch.rand(1) < self.epsilon and self.training:\n",
    "                num = self.k * self.dilation\n",
    "                randnum = torch.randperm(num)[:self.k]\n",
    "                edge_index = edge_index[:, :, :, randnum]\n",
    "            else:\n",
    "                edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        else:\n",
    "            edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        return edge_index\n",
    "\n",
    "\n",
    "class DenseDilatedKnnGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Find the neighbors' indices based on dilated knn\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilatedKnnGraph, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, y=None, relative_pos=None):\n",
    "        if y is not None:\n",
    "            #### normalize\n",
    "            x = F.normalize(x, p=2.0, dim=1)\n",
    "            y = F.normalize(y, p=2.0, dim=1)\n",
    "            ####\n",
    "            edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n",
    "        else:\n",
    "            #### normalize\n",
    "            x = F.normalize(x, p=2.0, dim=1)\n",
    "            ####\n",
    "            edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n",
    "        return self._dilated(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n",
    "\n",
    "\n",
    "##############################\n",
    "#    Basic layers\n",
    "##############################\n",
    "def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n",
    "    # activation layer\n",
    "\n",
    "    act = act.lower()\n",
    "    if act == 'relu':\n",
    "        layer = nn.ReLU(inplace)\n",
    "    elif act == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
    "    elif act == 'prelu':\n",
    "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
    "    elif act == 'gelu':\n",
    "        layer = nn.GELU()\n",
    "    elif act == 'hswish':\n",
    "        layer = nn.Hardswish(inplace)\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def norm_layer(norm, nc):\n",
    "    # normalization layer 2d\n",
    "    norm = norm.lower()\n",
    "    if norm == 'batch':\n",
    "        layer = nn.BatchNorm2d(nc, affine=True)\n",
    "    elif norm == 'instance':\n",
    "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class MLP(Seq):\n",
    "    def __init__(self, channels, act='relu', norm=None, bias=True):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Lin(channels[i - 1], channels[i], bias))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "        super(MLP, self).__init__(*m)\n",
    "\n",
    "\n",
    "class BasicConv(Seq):\n",
    "    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if drop > 0:\n",
    "                m.append(nn.Dropout2d(drop))\n",
    "\n",
    "        super(BasicConv, self).__init__(*m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def batched_index_select(x, idx):\n",
    "    r\"\"\"fetches neighbors features from a given neighbor idx\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): input feature Tensor\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n",
    "        idx (Tensor): edge_idx\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n",
    "    Returns:\n",
    "        Tensor: output neighbors features\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n",
    "    \"\"\"\n",
    "    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n",
    "    _, num_vertices, k = idx.shape\n",
    "    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.contiguous().view(-1)\n",
    "\n",
    "    x = x.transpose(2, 1)\n",
    "    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# from .torch_nn import BasicConv, batched_index_select, act_layer\n",
    "# from .torch_edge import DenseDilatedKnnGraph\n",
    "# from .pos_embed import get_2d_relative_pos_embed\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "\n",
    "class MRConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(MRConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n",
    "        b, c, n, _ = x.shape\n",
    "        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "class EdgeConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge convolution layer (with activation, batch normalization) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(EdgeConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n",
    "        return max_value\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)\n",
    "        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)\n",
    "        return self.nn2(torch.cat([x, x_j], dim=1))\n",
    "\n",
    "\n",
    "class GINConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(GINConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)\n",
    "        eps_init = 0.0\n",
    "        self.eps = nn.Parameter(torch.Tensor([eps_init]))\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j = torch.sum(x_j, -1, keepdim=True)\n",
    "        return self.nn((1 + self.eps) * x + x_j)\n",
    "\n",
    "\n",
    "class GraphConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Static graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n",
    "        super(GraphConv2d, self).__init__()\n",
    "        if conv == 'edge':\n",
    "            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'mr':\n",
    "            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'sage':\n",
    "            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'gin':\n",
    "            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        else:\n",
    "            raise NotImplementedError('conv:{} is not supported'.format(conv))\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        return self.gconv(x, edge_index, y)\n",
    "\n",
    "\n",
    "class DyGraphConv2d(GraphConv2d):\n",
    "    \"\"\"\n",
    "    Dynamic graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n",
    "                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n",
    "        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n",
    "        self.k = kernel_size\n",
    "        self.d = dilation\n",
    "        self.r = r\n",
    "        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, relative_pos=None):\n",
    "        B, C, H, W = x.shape\n",
    "        y = None\n",
    "        if self.r > 1:\n",
    "            y = F.avg_pool2d(x, self.r, self.r)\n",
    "            y = y.reshape(B, C, -1, 1).contiguous()            \n",
    "        x = x.reshape(B, C, -1, 1).contiguous()\n",
    "        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n",
    "        x = super(DyGraphConv2d, self).forward(x, edge_index, y)\n",
    "        return x.reshape(B, -1, H, W).contiguous()\n",
    "\n",
    "\n",
    "class Grapher(nn.Module):\n",
    "    \"\"\"\n",
    "    Grapher module with graph convolution and fc layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n",
    "                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n",
    "        super(Grapher, self).__init__()\n",
    "        self.channels = in_channels\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n",
    "                              act, norm, bias, stochastic, epsilon, r)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.relative_pos = None\n",
    "        if relative_pos:\n",
    "            print('using relative_pos')\n",
    "            relative_pos_tensor = torch.from_numpy(np.float32(get_2d_relative_pos_embed(in_channels,\n",
    "                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n",
    "            relative_pos_tensor = F.interpolate(\n",
    "                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n",
    "            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)\n",
    "\n",
    "    def _get_relative_pos(self, relative_pos, H, W):\n",
    "        if relative_pos is None or H * W == self.n:\n",
    "            return relative_pos\n",
    "        else:\n",
    "            N = H * W\n",
    "            N_reduced = N // (self.r * self.r)\n",
    "            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _tmp = x\n",
    "        x = self.fc1(x)\n",
    "        B, C, H, W = x.shape\n",
    "        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n",
    "        x = self.graph_conv(x, relative_pos)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + _tmp\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "# from gcn_lib import Grapher, act_layer\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'vig_224_gelu': _cfg(\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vig_b_224_gelu': _cfg(\n",
    "        crop_pct=0.95, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "        )\n",
    "        self.act = act_layer(act)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + shortcut\n",
    "        return x#.reshape(B, C, N, 1)\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    \"\"\" Image to Visual Embedding\n",
    "    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n",
    "        super().__init__()        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim//2, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim//2),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\" Convolution-based downsample\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=768):\n",
    "        super().__init__()        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        print(opt)\n",
    "        k = opt.k\n",
    "        act = opt.act\n",
    "        norm = opt.norm\n",
    "        bias = opt.bias\n",
    "        epsilon = opt.epsilon\n",
    "        stochastic = opt.use_stochastic\n",
    "        conv = opt.conv\n",
    "        emb_dims = opt.emb_dims\n",
    "        drop_path = opt.drop_path\n",
    "        \n",
    "        blocks = opt.blocks\n",
    "        self.n_blocks = sum(blocks)\n",
    "        channels = opt.channels\n",
    "        reduce_ratios = [4, 2, 1, 1]\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n",
    "        num_knn = [int(x.item()) for x in torch.linspace(k, k, self.n_blocks)]  # number of knn's k\n",
    "        max_dilation = 49 // max(num_knn)\n",
    "        \n",
    "        self.stem = Stem(out_dim=channels[0], act=act)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, channels[0], 224//4, 224//4))\n",
    "        HW = 224 // 4 * 224 // 4\n",
    "\n",
    "        self.backbone = nn.ModuleList([])\n",
    "        idx = 0\n",
    "        for i in range(len(blocks)):\n",
    "            if i > 0:\n",
    "                self.backbone.append(Downsample(channels[i-1], channels[i]))\n",
    "                HW = HW // 4\n",
    "            for j in range(blocks[i]):\n",
    "                self.backbone += [\n",
    "                    Seq(Grapher(channels[i], num_knn[idx], min(idx // 4 + 1, max_dilation), conv, act, norm,\n",
    "                                    bias, stochastic, epsilon, reduce_ratios[i], n=HW, drop_path=dpr[idx],\n",
    "                                    relative_pos=True),\n",
    "                          FFN(channels[i], channels[i] * 4, act=act, drop_path=dpr[idx])\n",
    "                         )]\n",
    "                idx += 1\n",
    "        self.backbone = Seq(*self.backbone)\n",
    "\n",
    "        self.prediction = Seq(nn.Conv2d(channels[-1], 1024, 1, bias=True),\n",
    "                              nn.BatchNorm2d(1024),\n",
    "                              act_layer(act),\n",
    "                              nn.Dropout(opt.dropout),\n",
    "                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.stem(inputs) + self.pos_embed\n",
    "        B, C, H, W = x.shape\n",
    "        for i in range(len(self.backbone)):\n",
    "            x = self.backbone[i](x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        return self.prediction(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_ti_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n",
    "            self.channels = [48, 96, 240, 384] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_s_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n",
    "            self.channels = [80, 160, 400, 640] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_m_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,16,2] # number of basic blocks in the backbone\n",
    "            self.channels = [96, 192, 384, 768] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_b_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,18,2] # number of basic blocks in the backbone\n",
    "            self.channels = [128, 256, 512, 1024] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_b_224_gelu']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터별 등급 분별 (특상 L, 상 M, 보통 S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5943 2388\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n",
    "# 검증을 위한 일반화\n",
    "# from email.mime import image\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = '../../Apple_Quality/dataset/Apple_sweetlevel_Jeonbuk/'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(len(image_datasets['train']), len(image_datasets['val']))\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "              for x in ['train', 'val']}\n",
    "            \n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arisu 등급 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.utils.data.Subset 모듈로 나누기 (20% validation)\n",
    "# val_size = int(0.2 * len(Arisu_datasets))\n",
    "# train_set = torch.utils.data.Subset(Arisu_datasets, range(val_size, len(Arisu_datasets)))\n",
    "# val_set = torch.utils.data.Subset(Arisu_datasets, range(val_size))\n",
    "\n",
    "# print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=4,\n",
    "#                                              shuffle=True, num_workers=0, drop_last=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=True, num_workers=0, drop_last=True)\n",
    "# # dataloaders['test'] = torch.utils.data.DataLoader(train_set, batch_size=16,\n",
    "#                                             #  shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.pvig_ti_224.<locals>.OptInit object at 0x7f423d2d1cd0>\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61438/3354495458.py:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using relative_pos\n",
      "using relative_pos\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mprediction[\u001b[39m4\u001b[39m] \u001b[39m=\u001b[39m Conv2d(\u001b[39m1024\u001b[39m, \u001b[39mlen\u001b[39m(class_names), kernel_size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), stride\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(model)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = pvig_ti_224()\n",
    "model.prediction[4] = Conv2d(1024, len(class_names), kernel_size=(1,1), stride=(1,1))\n",
    "# print(model)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sweet_B_grade', 'Sweet_A_grade', 'Sweet_C_grade', 'Sweet_B_grade', 'Sweet_B_grade', 'Sweet_C_grade', 'Sweet_C_grade', 'Sweet_A_grade']\n"
     ]
    }
   ],
   "source": [
    "# transform 된 배치 이미지 시각화\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n",
    "\n",
    "\n",
    "# 학습 데이터의 배치를 얻습니다.\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# 배치로부터 격자 형태의 이미지를 만듭니다.\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out)\n",
    "print([class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    # model = model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        # print(f\"time per 1 epoch: {time.time() - since:.2f}\")\n",
    "\n",
    "        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                # model.load_state_dict(model_state_dict)\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                # model.load_state_dict(model_state_dict)\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 데이터를 반복\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 매개변수 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "            writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
    "            # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n",
    "\n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                print(\"Update best acc! :\", best_acc)\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "       \n",
    "        print()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    # wandb.log({'test_acc': best_acc})\n",
    "    \n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.78 GiB total capacity; 702.49 MiB already allocated; 5.75 MiB free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb 셀 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(model, criterion, optimizer, exp_lr_scheduler,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32m/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb 셀 16\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, criterion, optimizer, scheduler, num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# model = model.to(device)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     since \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     best_model_wts \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(model\u001b[39m.\u001b[39;49mstate_dict())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     best_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.57/home/andrew/jihoon_python/pretrained_GNN/code/11_pretrainedViG_sweetlevel.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epochs)):\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/copy.py:296\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    295\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 296\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    297\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/_tensor.py:98\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     96\u001b[0m     new_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone()\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     new_storage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage()\u001b[39m.\u001b[39;49m__deepcopy__(memo)\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_quantized:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         quantizer_params: Union[Tuple[torch\u001b[39m.\u001b[39mqscheme, \u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m], Tuple[torch\u001b[39m.\u001b[39mqscheme, Tensor, Tensor, \u001b[39mint\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/storage.py:450\u001b[0m, in \u001b[0;36m_TypedStorage.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__deepcopy__\u001b[39m(\u001b[39mself\u001b[39m, memo):\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_wrapped_storage(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage, memo))\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/storage.py:59\u001b[0m, in \u001b[0;36m_StorageBase.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata]\n\u001b[0;32m---> 59\u001b[0m new_storage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone()\n\u001b[1;32m     60\u001b[0m memo[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata] \u001b[39m=\u001b[39m new_storage\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m new_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/storage.py:75\u001b[0m, in \u001b[0;36m_StorageBase.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_device() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cuda \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnbytes())\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/geo/lib/python3.9/site-packages/torch/cuda/__init__.py:661\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m _lazy_init()\n\u001b[1;32m    659\u001b[0m \u001b[39m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_CudaBase, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.78 GiB total capacity; 702.49 MiB already allocated; 5.75 MiB free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8616a860257f6741c1b5f4a77ff6ccf51a19eff03d81799217c8f280496915f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
