{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f3cc9c2ad00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # 대화형 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model 불러오기\n",
    "path = '../model/pretrained/pvig_b.pth.tar'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_state_dict = torch.load(path, map_location= device)        \n",
    "# print(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "# modified from https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# --------------------------------------------------------\n",
    "# Position embedding utils\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# relative position embedding\n",
    "# References: https://arxiv.org/abs/2009.13658\n",
    "# --------------------------------------------------------\n",
    "def get_2d_relative_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, grid_size*grid_size]\n",
    "    \"\"\"\n",
    "    pos_embed = get_2d_sincos_pos_embed(embed_dim, grid_size)\n",
    "    relative_pos = 2 * np.matmul(pos_embed, pos_embed.transpose()) / pos_embed.shape[1]\n",
    "    return relative_pos\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def pairwise_distance(x):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square + x_inner + x_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def part_pairwise_distance(x, start_idx=0, end_idx=1):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_part = x[:, start_idx:end_idx]\n",
    "        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n",
    "        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square_part + x_inner + x_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def xy_pairwise_distance(x, y):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        xy_inner = -2*torch.matmul(x, y.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        y_square = torch.sum(torch.mul(y, y), dim=-1, keepdim=True)\n",
    "        return x_square + xy_inner + y_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def dense_knn_matrix(x, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.transpose(2, 1).squeeze(-1)\n",
    "        batch_size, n_points, n_dims = x.shape\n",
    "        ### memory efficient implementation ###\n",
    "        n_part = 10000\n",
    "        if n_points > n_part:\n",
    "            nn_idx_list = []\n",
    "            groups = math.ceil(n_points / n_part)\n",
    "            for i in range(groups):\n",
    "                start_idx = n_part * i\n",
    "                end_idx = min(n_points, n_part * (i + 1))\n",
    "                dist = part_pairwise_distance(x.detach(), start_idx, end_idx)\n",
    "                if relative_pos is not None:\n",
    "                    dist += relative_pos[:, start_idx:end_idx]\n",
    "                _, nn_idx_part = torch.topk(-dist, k=k)\n",
    "                nn_idx_list += [nn_idx_part]\n",
    "            nn_idx = torch.cat(nn_idx_list, dim=1)\n",
    "        else:\n",
    "            dist = pairwise_distance(x.detach())\n",
    "            if relative_pos is not None:\n",
    "                dist += relative_pos\n",
    "            _, nn_idx = torch.topk(-dist, k=k) # b, n, k\n",
    "        ######\n",
    "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
    "    return torch.stack((nn_idx, center_idx), dim=0)\n",
    "\n",
    "\n",
    "def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.transpose(2, 1).squeeze(-1)\n",
    "        y = y.transpose(2, 1).squeeze(-1)\n",
    "        batch_size, n_points, n_dims = x.shape\n",
    "        dist = xy_pairwise_distance(x.detach(), y.detach())\n",
    "        if relative_pos is not None:\n",
    "            dist += relative_pos\n",
    "        _, nn_idx = torch.topk(-dist, k=k)\n",
    "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
    "    return torch.stack((nn_idx, center_idx), dim=0)\n",
    "\n",
    "\n",
    "class DenseDilated(nn.Module):\n",
    "    \"\"\"\n",
    "    Find dilated neighbor from neighbor list\n",
    "\n",
    "    edge_index: (2, batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilated, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        if self.stochastic:\n",
    "            if torch.rand(1) < self.epsilon and self.training:\n",
    "                num = self.k * self.dilation\n",
    "                randnum = torch.randperm(num)[:self.k]\n",
    "                edge_index = edge_index[:, :, :, randnum]\n",
    "            else:\n",
    "                edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        else:\n",
    "            edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        return edge_index\n",
    "\n",
    "\n",
    "class DenseDilatedKnnGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Find the neighbors' indices based on dilated knn\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilatedKnnGraph, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, y=None, relative_pos=None):\n",
    "        if y is not None:\n",
    "            #### normalize\n",
    "            x = F.normalize(x, p=2.0, dim=1)\n",
    "            y = F.normalize(y, p=2.0, dim=1)\n",
    "            ####\n",
    "            edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n",
    "        else:\n",
    "            #### normalize\n",
    "            x = F.normalize(x, p=2.0, dim=1)\n",
    "            ####\n",
    "            edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n",
    "        return self._dilated(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n",
    "\n",
    "\n",
    "##############################\n",
    "#    Basic layers\n",
    "##############################\n",
    "def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n",
    "    # activation layer\n",
    "\n",
    "    act = act.lower()\n",
    "    if act == 'relu':\n",
    "        layer = nn.ReLU(inplace)\n",
    "    elif act == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
    "    elif act == 'prelu':\n",
    "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
    "    elif act == 'gelu':\n",
    "        layer = nn.GELU()\n",
    "    elif act == 'hswish':\n",
    "        layer = nn.Hardswish(inplace)\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def norm_layer(norm, nc):\n",
    "    # normalization layer 2d\n",
    "    norm = norm.lower()\n",
    "    if norm == 'batch':\n",
    "        layer = nn.BatchNorm2d(nc, affine=True)\n",
    "    elif norm == 'instance':\n",
    "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class MLP(Seq):\n",
    "    def __init__(self, channels, act='relu', norm=None, bias=True):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Lin(channels[i - 1], channels[i], bias))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "        super(MLP, self).__init__(*m)\n",
    "\n",
    "\n",
    "class BasicConv(Seq):\n",
    "    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if drop > 0:\n",
    "                m.append(nn.Dropout2d(drop))\n",
    "\n",
    "        super(BasicConv, self).__init__(*m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def batched_index_select(x, idx):\n",
    "    r\"\"\"fetches neighbors features from a given neighbor idx\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): input feature Tensor\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n",
    "        idx (Tensor): edge_idx\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n",
    "    Returns:\n",
    "        Tensor: output neighbors features\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n",
    "    \"\"\"\n",
    "    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n",
    "    _, num_vertices, k = idx.shape\n",
    "    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.contiguous().view(-1)\n",
    "\n",
    "    x = x.transpose(2, 1)\n",
    "    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# from .torch_nn import BasicConv, batched_index_select, act_layer\n",
    "# from .torch_edge import DenseDilatedKnnGraph\n",
    "# from .pos_embed import get_2d_relative_pos_embed\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "\n",
    "class MRConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(MRConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n",
    "        b, c, n, _ = x.shape\n",
    "        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "class EdgeConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge convolution layer (with activation, batch normalization) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(EdgeConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n",
    "        return max_value\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)\n",
    "        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)\n",
    "        return self.nn2(torch.cat([x, x_j], dim=1))\n",
    "\n",
    "\n",
    "class GINConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(GINConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)\n",
    "        eps_init = 0.0\n",
    "        self.eps = nn.Parameter(torch.Tensor([eps_init]))\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j = torch.sum(x_j, -1, keepdim=True)\n",
    "        return self.nn((1 + self.eps) * x + x_j)\n",
    "\n",
    "\n",
    "class GraphConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Static graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n",
    "        super(GraphConv2d, self).__init__()\n",
    "        if conv == 'edge':\n",
    "            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'mr':\n",
    "            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'sage':\n",
    "            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'gin':\n",
    "            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        else:\n",
    "            raise NotImplementedError('conv:{} is not supported'.format(conv))\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        return self.gconv(x, edge_index, y)\n",
    "\n",
    "\n",
    "class DyGraphConv2d(GraphConv2d):\n",
    "    \"\"\"\n",
    "    Dynamic graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n",
    "                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n",
    "        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n",
    "        self.k = kernel_size\n",
    "        self.d = dilation\n",
    "        self.r = r\n",
    "        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, relative_pos=None):\n",
    "        B, C, H, W = x.shape\n",
    "        y = None\n",
    "        if self.r > 1:\n",
    "            y = F.avg_pool2d(x, self.r, self.r)\n",
    "            y = y.reshape(B, C, -1, 1).contiguous()            \n",
    "        x = x.reshape(B, C, -1, 1).contiguous()\n",
    "        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n",
    "        x = super(DyGraphConv2d, self).forward(x, edge_index, y)\n",
    "        return x.reshape(B, -1, H, W).contiguous()\n",
    "\n",
    "\n",
    "class Grapher(nn.Module):\n",
    "    \"\"\"\n",
    "    Grapher module with graph convolution and fc layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n",
    "                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n",
    "        super(Grapher, self).__init__()\n",
    "        self.channels = in_channels\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n",
    "                              act, norm, bias, stochastic, epsilon, r)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.relative_pos = None\n",
    "        if relative_pos:\n",
    "            print('using relative_pos')\n",
    "            relative_pos_tensor = torch.from_numpy(np.float32(get_2d_relative_pos_embed(in_channels,\n",
    "                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n",
    "            relative_pos_tensor = F.interpolate(\n",
    "                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n",
    "            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)\n",
    "\n",
    "    def _get_relative_pos(self, relative_pos, H, W):\n",
    "        if relative_pos is None or H * W == self.n:\n",
    "            return relative_pos\n",
    "        else:\n",
    "            N = H * W\n",
    "            N_reduced = N // (self.r * self.r)\n",
    "            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _tmp = x\n",
    "        x = self.fc1(x)\n",
    "        B, C, H, W = x.shape\n",
    "        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n",
    "        x = self.graph_conv(x, relative_pos)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + _tmp\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2022.06.17-Changed for building ViG model\n",
    "#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "# from gcn_lib import Grapher, act_layer\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'vig_224_gelu': _cfg(\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vig_b_224_gelu': _cfg(\n",
    "        crop_pct=0.95, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "        )\n",
    "        self.act = act_layer(act)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + shortcut\n",
    "        return x#.reshape(B, C, N, 1)\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    \"\"\" Image to Visual Embedding\n",
    "    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n",
    "        super().__init__()        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim//2, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim//2),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\" Convolution-based downsample\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, out_dim=768):\n",
    "        super().__init__()        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        print(opt)\n",
    "        k = opt.k\n",
    "        act = opt.act\n",
    "        norm = opt.norm\n",
    "        bias = opt.bias\n",
    "        epsilon = opt.epsilon\n",
    "        stochastic = opt.use_stochastic\n",
    "        conv = opt.conv\n",
    "        emb_dims = opt.emb_dims\n",
    "        drop_path = opt.drop_path\n",
    "        \n",
    "        blocks = opt.blocks\n",
    "        self.n_blocks = sum(blocks)\n",
    "        channels = opt.channels\n",
    "        reduce_ratios = [4, 2, 1, 1]\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n",
    "        num_knn = [int(x.item()) for x in torch.linspace(k, k, self.n_blocks)]  # number of knn's k\n",
    "        max_dilation = 49 // max(num_knn)\n",
    "        \n",
    "        self.stem = Stem(out_dim=channels[0], act=act)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, channels[0], 224//4, 224//4))\n",
    "        HW = 224 // 4 * 224 // 4\n",
    "\n",
    "        self.backbone = nn.ModuleList([])\n",
    "        idx = 0\n",
    "        for i in range(len(blocks)):\n",
    "            if i > 0:\n",
    "                self.backbone.append(Downsample(channels[i-1], channels[i]))\n",
    "                HW = HW // 4\n",
    "            for j in range(blocks[i]):\n",
    "                self.backbone += [\n",
    "                    Seq(Grapher(channels[i], num_knn[idx], min(idx // 4 + 1, max_dilation), conv, act, norm,\n",
    "                                    bias, stochastic, epsilon, reduce_ratios[i], n=HW, drop_path=dpr[idx],\n",
    "                                    relative_pos=True),\n",
    "                          FFN(channels[i], channels[i] * 4, act=act, drop_path=dpr[idx])\n",
    "                         )]\n",
    "                idx += 1\n",
    "        self.backbone = Seq(*self.backbone)\n",
    "\n",
    "        self.prediction = Seq(nn.Conv2d(channels[-1], 1024, 1, bias=True),\n",
    "                              nn.BatchNorm2d(1024),\n",
    "                              act_layer(act),\n",
    "                              nn.Dropout(opt.dropout),\n",
    "                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.stem(inputs) + self.pos_embed\n",
    "        B, C, H, W = x.shape\n",
    "        for i in range(len(self.backbone)):\n",
    "            x = self.backbone[i](x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        return self.prediction(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_ti_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n",
    "            self.channels = [48, 96, 240, 384] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_s_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n",
    "            self.channels = [80, 160, 400, 640] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_m_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,16,2] # number of basic blocks in the backbone\n",
    "            self.channels = [96, 192, 384, 768] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def pvig_b_224(pretrained=False, **kwargs):\n",
    "    class OptInit:\n",
    "        def __init__(self, num_classes=1000, drop_path_rate=0.0, **kwargs):\n",
    "            self.k = 9 # neighbor num (default:9)\n",
    "            self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "            self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "            self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "            self.bias = True # bias of conv layer True or False\n",
    "            self.dropout = 0.0 # dropout rate\n",
    "            self.use_dilation = True # use dilated knn or not\n",
    "            self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "            self.use_stochastic = False # stochastic for gcn, True or False\n",
    "            self.drop_path = drop_path_rate\n",
    "            self.blocks = [2,2,18,2] # number of basic blocks in the backbone\n",
    "            self.channels = [128, 256, 512, 1024] # number of channels of deep features\n",
    "            self.n_classes = num_classes # Dimension of out_channels\n",
    "            self.emb_dims = 1024 # Dimension of embeddings\n",
    "\n",
    "    opt = OptInit(**kwargs)\n",
    "    model = DeepGCN(opt)\n",
    "    model.default_cfg = default_cfgs['vig_b_224_gelu']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.pvig_b_224.<locals>.OptInit object at 0x7f3c806a43d0>\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31005/3354495458.py:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n",
      "using relative_pos\n"
     ]
    }
   ],
   "source": [
    "model = pvig_b_224()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              GELU-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4          [-1, 128, 56, 56]          73,856\n",
      "       BatchNorm2d-5          [-1, 128, 56, 56]             256\n",
      "              GELU-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 128, 56, 56]         147,584\n",
      "       BatchNorm2d-8          [-1, 128, 56, 56]             256\n",
      "              Stem-9          [-1, 128, 56, 56]               0\n",
      "           Conv2d-10          [-1, 128, 56, 56]          16,512\n",
      "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
      "     DenseDilated-12           [-1, 2, 3136, 9]               0\n",
      "DenseDilatedKnnGraph-13           [-1, 2, 3136, 9]               0\n",
      "           Conv2d-14         [-1, 256, 3136, 1]          16,640\n",
      "      BatchNorm2d-15         [-1, 256, 3136, 1]             512\n",
      "             GELU-16         [-1, 256, 3136, 1]               0\n",
      "         MRConv2d-17         [-1, 256, 3136, 1]               0\n",
      "    DyGraphConv2d-18          [-1, 256, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          32,896\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "         Identity-21          [-1, 128, 56, 56]               0\n",
      "          Grapher-22          [-1, 128, 56, 56]               0\n",
      "           Conv2d-23          [-1, 512, 56, 56]          66,048\n",
      "      BatchNorm2d-24          [-1, 512, 56, 56]           1,024\n",
      "             GELU-25          [-1, 512, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 56, 56]          65,664\n",
      "      BatchNorm2d-27          [-1, 128, 56, 56]             256\n",
      "         Identity-28          [-1, 128, 56, 56]               0\n",
      "              FFN-29          [-1, 128, 56, 56]               0\n",
      "           Conv2d-30          [-1, 128, 56, 56]          16,512\n",
      "      BatchNorm2d-31          [-1, 128, 56, 56]             256\n",
      "     DenseDilated-32           [-1, 2, 3136, 9]               0\n",
      "DenseDilatedKnnGraph-33           [-1, 2, 3136, 9]               0\n",
      "           Conv2d-34         [-1, 256, 3136, 1]          16,640\n",
      "      BatchNorm2d-35         [-1, 256, 3136, 1]             512\n",
      "             GELU-36         [-1, 256, 3136, 1]               0\n",
      "         MRConv2d-37         [-1, 256, 3136, 1]               0\n",
      "    DyGraphConv2d-38          [-1, 256, 56, 56]               0\n",
      "           Conv2d-39          [-1, 128, 56, 56]          32,896\n",
      "      BatchNorm2d-40          [-1, 128, 56, 56]             256\n",
      "         Identity-41          [-1, 128, 56, 56]               0\n",
      "          Grapher-42          [-1, 128, 56, 56]               0\n",
      "           Conv2d-43          [-1, 512, 56, 56]          66,048\n",
      "      BatchNorm2d-44          [-1, 512, 56, 56]           1,024\n",
      "             GELU-45          [-1, 512, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          65,664\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "         Identity-48          [-1, 128, 56, 56]               0\n",
      "              FFN-49          [-1, 128, 56, 56]               0\n",
      "           Conv2d-50          [-1, 256, 28, 28]         295,168\n",
      "      BatchNorm2d-51          [-1, 256, 28, 28]             512\n",
      "       Downsample-52          [-1, 256, 28, 28]               0\n",
      "           Conv2d-53          [-1, 256, 28, 28]          65,792\n",
      "      BatchNorm2d-54          [-1, 256, 28, 28]             512\n",
      "     DenseDilated-55            [-1, 2, 784, 9]               0\n",
      "DenseDilatedKnnGraph-56            [-1, 2, 784, 9]               0\n",
      "           Conv2d-57          [-1, 512, 784, 1]          66,048\n",
      "      BatchNorm2d-58          [-1, 512, 784, 1]           1,024\n",
      "             GELU-59          [-1, 512, 784, 1]               0\n",
      "         MRConv2d-60          [-1, 512, 784, 1]               0\n",
      "    DyGraphConv2d-61          [-1, 512, 28, 28]               0\n",
      "           Conv2d-62          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-63          [-1, 256, 28, 28]             512\n",
      "         Identity-64          [-1, 256, 28, 28]               0\n",
      "          Grapher-65          [-1, 256, 28, 28]               0\n",
      "           Conv2d-66         [-1, 1024, 28, 28]         263,168\n",
      "      BatchNorm2d-67         [-1, 1024, 28, 28]           2,048\n",
      "             GELU-68         [-1, 1024, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 28, 28]         262,400\n",
      "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
      "         Identity-71          [-1, 256, 28, 28]               0\n",
      "              FFN-72          [-1, 256, 28, 28]               0\n",
      "           Conv2d-73          [-1, 256, 28, 28]          65,792\n",
      "      BatchNorm2d-74          [-1, 256, 28, 28]             512\n",
      "     DenseDilated-75            [-1, 2, 784, 9]               0\n",
      "DenseDilatedKnnGraph-76            [-1, 2, 784, 9]               0\n",
      "           Conv2d-77          [-1, 512, 784, 1]          66,048\n",
      "      BatchNorm2d-78          [-1, 512, 784, 1]           1,024\n",
      "             GELU-79          [-1, 512, 784, 1]               0\n",
      "         MRConv2d-80          [-1, 512, 784, 1]               0\n",
      "    DyGraphConv2d-81          [-1, 512, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-83          [-1, 256, 28, 28]             512\n",
      "         Identity-84          [-1, 256, 28, 28]               0\n",
      "          Grapher-85          [-1, 256, 28, 28]               0\n",
      "           Conv2d-86         [-1, 1024, 28, 28]         263,168\n",
      "      BatchNorm2d-87         [-1, 1024, 28, 28]           2,048\n",
      "             GELU-88         [-1, 1024, 28, 28]               0\n",
      "           Conv2d-89          [-1, 256, 28, 28]         262,400\n",
      "      BatchNorm2d-90          [-1, 256, 28, 28]             512\n",
      "         Identity-91          [-1, 256, 28, 28]               0\n",
      "              FFN-92          [-1, 256, 28, 28]               0\n",
      "           Conv2d-93          [-1, 512, 14, 14]       1,180,160\n",
      "      BatchNorm2d-94          [-1, 512, 14, 14]           1,024\n",
      "       Downsample-95          [-1, 512, 14, 14]               0\n",
      "           Conv2d-96          [-1, 512, 14, 14]         262,656\n",
      "      BatchNorm2d-97          [-1, 512, 14, 14]           1,024\n",
      "     DenseDilated-98            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-99            [-1, 2, 196, 9]               0\n",
      "          Conv2d-100         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-101         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-102         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-103         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-104         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-105          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-106          [-1, 512, 14, 14]           1,024\n",
      "        Identity-107          [-1, 512, 14, 14]               0\n",
      "         Grapher-108          [-1, 512, 14, 14]               0\n",
      "          Conv2d-109         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-110         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-111         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-112          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-113          [-1, 512, 14, 14]           1,024\n",
      "        Identity-114          [-1, 512, 14, 14]               0\n",
      "             FFN-115          [-1, 512, 14, 14]               0\n",
      "          Conv2d-116          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-117          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-118            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-119            [-1, 2, 196, 9]               0\n",
      "          Conv2d-120         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-121         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-122         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-123         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-124         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-125          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-126          [-1, 512, 14, 14]           1,024\n",
      "        Identity-127          [-1, 512, 14, 14]               0\n",
      "         Grapher-128          [-1, 512, 14, 14]               0\n",
      "          Conv2d-129         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-130         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-131         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-132          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-133          [-1, 512, 14, 14]           1,024\n",
      "        Identity-134          [-1, 512, 14, 14]               0\n",
      "             FFN-135          [-1, 512, 14, 14]               0\n",
      "          Conv2d-136          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-137          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-138            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-139            [-1, 2, 196, 9]               0\n",
      "          Conv2d-140         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-141         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-142         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-143         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-144         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-145          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n",
      "        Identity-147          [-1, 512, 14, 14]               0\n",
      "         Grapher-148          [-1, 512, 14, 14]               0\n",
      "          Conv2d-149         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-150         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-151         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-152          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-153          [-1, 512, 14, 14]           1,024\n",
      "        Identity-154          [-1, 512, 14, 14]               0\n",
      "             FFN-155          [-1, 512, 14, 14]               0\n",
      "          Conv2d-156          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-157          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-158            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-159            [-1, 2, 196, 9]               0\n",
      "          Conv2d-160         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-161         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-162         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-163         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-164         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-165          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-166          [-1, 512, 14, 14]           1,024\n",
      "        Identity-167          [-1, 512, 14, 14]               0\n",
      "         Grapher-168          [-1, 512, 14, 14]               0\n",
      "          Conv2d-169         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-170         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-171         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-172          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-173          [-1, 512, 14, 14]           1,024\n",
      "        Identity-174          [-1, 512, 14, 14]               0\n",
      "             FFN-175          [-1, 512, 14, 14]               0\n",
      "          Conv2d-176          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-177          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-178            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-179            [-1, 2, 196, 9]               0\n",
      "          Conv2d-180         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-181         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-182         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-183         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-184         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-185          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-186          [-1, 512, 14, 14]           1,024\n",
      "        Identity-187          [-1, 512, 14, 14]               0\n",
      "         Grapher-188          [-1, 512, 14, 14]               0\n",
      "          Conv2d-189         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-190         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-191         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-192          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
      "        Identity-194          [-1, 512, 14, 14]               0\n",
      "             FFN-195          [-1, 512, 14, 14]               0\n",
      "          Conv2d-196          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-197          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-198            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-199            [-1, 2, 196, 9]               0\n",
      "          Conv2d-200         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-201         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-202         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-203         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-204         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-205          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-206          [-1, 512, 14, 14]           1,024\n",
      "        Identity-207          [-1, 512, 14, 14]               0\n",
      "         Grapher-208          [-1, 512, 14, 14]               0\n",
      "          Conv2d-209         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-210         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-211         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-212          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-213          [-1, 512, 14, 14]           1,024\n",
      "        Identity-214          [-1, 512, 14, 14]               0\n",
      "             FFN-215          [-1, 512, 14, 14]               0\n",
      "          Conv2d-216          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-217          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-218            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-219            [-1, 2, 196, 9]               0\n",
      "          Conv2d-220         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-221         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-222         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-223         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-224         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-225          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-226          [-1, 512, 14, 14]           1,024\n",
      "        Identity-227          [-1, 512, 14, 14]               0\n",
      "         Grapher-228          [-1, 512, 14, 14]               0\n",
      "          Conv2d-229         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-230         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-231         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-232          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-233          [-1, 512, 14, 14]           1,024\n",
      "        Identity-234          [-1, 512, 14, 14]               0\n",
      "             FFN-235          [-1, 512, 14, 14]               0\n",
      "          Conv2d-236          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-237          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-238            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-239            [-1, 2, 196, 9]               0\n",
      "          Conv2d-240         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-241         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-242         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-243         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-244         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-245          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-246          [-1, 512, 14, 14]           1,024\n",
      "        Identity-247          [-1, 512, 14, 14]               0\n",
      "         Grapher-248          [-1, 512, 14, 14]               0\n",
      "          Conv2d-249         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-250         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-251         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-252          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-253          [-1, 512, 14, 14]           1,024\n",
      "        Identity-254          [-1, 512, 14, 14]               0\n",
      "             FFN-255          [-1, 512, 14, 14]               0\n",
      "          Conv2d-256          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-257          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-258            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-259            [-1, 2, 196, 9]               0\n",
      "          Conv2d-260         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-261         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-262         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-263         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-264         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-265          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-266          [-1, 512, 14, 14]           1,024\n",
      "        Identity-267          [-1, 512, 14, 14]               0\n",
      "         Grapher-268          [-1, 512, 14, 14]               0\n",
      "          Conv2d-269         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-270         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-271         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-272          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-273          [-1, 512, 14, 14]           1,024\n",
      "        Identity-274          [-1, 512, 14, 14]               0\n",
      "             FFN-275          [-1, 512, 14, 14]               0\n",
      "          Conv2d-276          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-277          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-278            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-279            [-1, 2, 196, 9]               0\n",
      "          Conv2d-280         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-281         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-282         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-283         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-284         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-285          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-286          [-1, 512, 14, 14]           1,024\n",
      "        Identity-287          [-1, 512, 14, 14]               0\n",
      "         Grapher-288          [-1, 512, 14, 14]               0\n",
      "          Conv2d-289         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-290         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-291         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-292          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-293          [-1, 512, 14, 14]           1,024\n",
      "        Identity-294          [-1, 512, 14, 14]               0\n",
      "             FFN-295          [-1, 512, 14, 14]               0\n",
      "          Conv2d-296          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-297          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-298            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-299            [-1, 2, 196, 9]               0\n",
      "          Conv2d-300         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-301         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-302         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-303         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-304         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-305          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-306          [-1, 512, 14, 14]           1,024\n",
      "        Identity-307          [-1, 512, 14, 14]               0\n",
      "         Grapher-308          [-1, 512, 14, 14]               0\n",
      "          Conv2d-309         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-310         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-311         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-312          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-313          [-1, 512, 14, 14]           1,024\n",
      "        Identity-314          [-1, 512, 14, 14]               0\n",
      "             FFN-315          [-1, 512, 14, 14]               0\n",
      "          Conv2d-316          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-317          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-318            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-319            [-1, 2, 196, 9]               0\n",
      "          Conv2d-320         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-321         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-322         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-323         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-324         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-325          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-326          [-1, 512, 14, 14]           1,024\n",
      "        Identity-327          [-1, 512, 14, 14]               0\n",
      "         Grapher-328          [-1, 512, 14, 14]               0\n",
      "          Conv2d-329         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-330         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-331         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-332          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-333          [-1, 512, 14, 14]           1,024\n",
      "        Identity-334          [-1, 512, 14, 14]               0\n",
      "             FFN-335          [-1, 512, 14, 14]               0\n",
      "          Conv2d-336          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-337          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-338            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-339            [-1, 2, 196, 9]               0\n",
      "          Conv2d-340         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-341         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-342         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-343         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-344         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-345          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-346          [-1, 512, 14, 14]           1,024\n",
      "        Identity-347          [-1, 512, 14, 14]               0\n",
      "         Grapher-348          [-1, 512, 14, 14]               0\n",
      "          Conv2d-349         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-350         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-351         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-352          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-353          [-1, 512, 14, 14]           1,024\n",
      "        Identity-354          [-1, 512, 14, 14]               0\n",
      "             FFN-355          [-1, 512, 14, 14]               0\n",
      "          Conv2d-356          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-357          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-358            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-359            [-1, 2, 196, 9]               0\n",
      "          Conv2d-360         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-361         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-362         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-363         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-364         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-365          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-366          [-1, 512, 14, 14]           1,024\n",
      "        Identity-367          [-1, 512, 14, 14]               0\n",
      "         Grapher-368          [-1, 512, 14, 14]               0\n",
      "          Conv2d-369         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-370         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-371         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-372          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-373          [-1, 512, 14, 14]           1,024\n",
      "        Identity-374          [-1, 512, 14, 14]               0\n",
      "             FFN-375          [-1, 512, 14, 14]               0\n",
      "          Conv2d-376          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-377          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-378            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-379            [-1, 2, 196, 9]               0\n",
      "          Conv2d-380         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-381         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-382         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-383         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-384         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-385          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-386          [-1, 512, 14, 14]           1,024\n",
      "        Identity-387          [-1, 512, 14, 14]               0\n",
      "         Grapher-388          [-1, 512, 14, 14]               0\n",
      "          Conv2d-389         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-390         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-391         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-392          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-393          [-1, 512, 14, 14]           1,024\n",
      "        Identity-394          [-1, 512, 14, 14]               0\n",
      "             FFN-395          [-1, 512, 14, 14]               0\n",
      "          Conv2d-396          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-397          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-398            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-399            [-1, 2, 196, 9]               0\n",
      "          Conv2d-400         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-401         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-402         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-403         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-404         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-405          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-406          [-1, 512, 14, 14]           1,024\n",
      "        Identity-407          [-1, 512, 14, 14]               0\n",
      "         Grapher-408          [-1, 512, 14, 14]               0\n",
      "          Conv2d-409         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-410         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-411         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-412          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-413          [-1, 512, 14, 14]           1,024\n",
      "        Identity-414          [-1, 512, 14, 14]               0\n",
      "             FFN-415          [-1, 512, 14, 14]               0\n",
      "          Conv2d-416          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-417          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-418            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-419            [-1, 2, 196, 9]               0\n",
      "          Conv2d-420         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-421         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-422         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-423         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-424         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-425          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-426          [-1, 512, 14, 14]           1,024\n",
      "        Identity-427          [-1, 512, 14, 14]               0\n",
      "         Grapher-428          [-1, 512, 14, 14]               0\n",
      "          Conv2d-429         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-430         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-431         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-432          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-433          [-1, 512, 14, 14]           1,024\n",
      "        Identity-434          [-1, 512, 14, 14]               0\n",
      "             FFN-435          [-1, 512, 14, 14]               0\n",
      "          Conv2d-436          [-1, 512, 14, 14]         262,656\n",
      "     BatchNorm2d-437          [-1, 512, 14, 14]           1,024\n",
      "    DenseDilated-438            [-1, 2, 196, 9]               0\n",
      "DenseDilatedKnnGraph-439            [-1, 2, 196, 9]               0\n",
      "          Conv2d-440         [-1, 1024, 196, 1]         263,168\n",
      "     BatchNorm2d-441         [-1, 1024, 196, 1]           2,048\n",
      "            GELU-442         [-1, 1024, 196, 1]               0\n",
      "        MRConv2d-443         [-1, 1024, 196, 1]               0\n",
      "   DyGraphConv2d-444         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-445          [-1, 512, 14, 14]         524,800\n",
      "     BatchNorm2d-446          [-1, 512, 14, 14]           1,024\n",
      "        Identity-447          [-1, 512, 14, 14]               0\n",
      "         Grapher-448          [-1, 512, 14, 14]               0\n",
      "          Conv2d-449         [-1, 2048, 14, 14]       1,050,624\n",
      "     BatchNorm2d-450         [-1, 2048, 14, 14]           4,096\n",
      "            GELU-451         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-452          [-1, 512, 14, 14]       1,049,088\n",
      "     BatchNorm2d-453          [-1, 512, 14, 14]           1,024\n",
      "        Identity-454          [-1, 512, 14, 14]               0\n",
      "             FFN-455          [-1, 512, 14, 14]               0\n",
      "          Conv2d-456           [-1, 1024, 7, 7]       4,719,616\n",
      "     BatchNorm2d-457           [-1, 1024, 7, 7]           2,048\n",
      "      Downsample-458           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-459           [-1, 1024, 7, 7]       1,049,600\n",
      "     BatchNorm2d-460           [-1, 1024, 7, 7]           2,048\n",
      "    DenseDilated-461             [-1, 2, 49, 9]               0\n",
      "DenseDilatedKnnGraph-462             [-1, 2, 49, 9]               0\n",
      "          Conv2d-463          [-1, 2048, 49, 1]       1,050,624\n",
      "     BatchNorm2d-464          [-1, 2048, 49, 1]           4,096\n",
      "            GELU-465          [-1, 2048, 49, 1]               0\n",
      "        MRConv2d-466          [-1, 2048, 49, 1]               0\n",
      "   DyGraphConv2d-467           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-468           [-1, 1024, 7, 7]       2,098,176\n",
      "     BatchNorm2d-469           [-1, 1024, 7, 7]           2,048\n",
      "        Identity-470           [-1, 1024, 7, 7]               0\n",
      "         Grapher-471           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-472           [-1, 4096, 7, 7]       4,198,400\n",
      "     BatchNorm2d-473           [-1, 4096, 7, 7]           8,192\n",
      "            GELU-474           [-1, 4096, 7, 7]               0\n",
      "          Conv2d-475           [-1, 1024, 7, 7]       4,195,328\n",
      "     BatchNorm2d-476           [-1, 1024, 7, 7]           2,048\n",
      "        Identity-477           [-1, 1024, 7, 7]               0\n",
      "             FFN-478           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-479           [-1, 1024, 7, 7]       1,049,600\n",
      "     BatchNorm2d-480           [-1, 1024, 7, 7]           2,048\n",
      "    DenseDilated-481             [-1, 2, 49, 9]               0\n",
      "DenseDilatedKnnGraph-482             [-1, 2, 49, 9]               0\n",
      "          Conv2d-483          [-1, 2048, 49, 1]       1,050,624\n",
      "     BatchNorm2d-484          [-1, 2048, 49, 1]           4,096\n",
      "            GELU-485          [-1, 2048, 49, 1]               0\n",
      "        MRConv2d-486          [-1, 2048, 49, 1]               0\n",
      "   DyGraphConv2d-487           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-488           [-1, 1024, 7, 7]       2,098,176\n",
      "     BatchNorm2d-489           [-1, 1024, 7, 7]           2,048\n",
      "        Identity-490           [-1, 1024, 7, 7]               0\n",
      "         Grapher-491           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-492           [-1, 4096, 7, 7]       4,198,400\n",
      "     BatchNorm2d-493           [-1, 4096, 7, 7]           8,192\n",
      "            GELU-494           [-1, 4096, 7, 7]               0\n",
      "          Conv2d-495           [-1, 1024, 7, 7]       4,195,328\n",
      "     BatchNorm2d-496           [-1, 1024, 7, 7]           2,048\n",
      "        Identity-497           [-1, 1024, 7, 7]               0\n",
      "             FFN-498           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-499           [-1, 1024, 1, 1]       1,049,600\n",
      "     BatchNorm2d-500           [-1, 1024, 1, 1]           2,048\n",
      "            GELU-501           [-1, 1024, 1, 1]               0\n",
      "         Dropout-502           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-503           [-1, 1000, 1, 1]       1,025,000\n",
      "================================================================\n",
      "Total params: 92,578,920\n",
      "Trainable params: 92,578,920\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 807.48\n",
      "Params size (MB): 353.16\n",
      "Estimated Total Size (MB): 1161.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# input = torch.Tensor([(3, 224, 224)], device=dev)\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14758 3694\n"
     ]
    }
   ],
   "source": [
    "# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n",
    "# 검증을 위한 일반화\n",
    "# from email.mime import image\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = '../dataset/fruit_quality/'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "print(len(image_datasets['train']), len(image_datasets['val']))\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# # StratifiedShuffleSplit sample the data in same proportion of labels\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "# indices = list(range(len(image_datasets['train'])))          # test set으로부터 나누고싶으면 'train' -> 'test'으로 변경 / 대신 레이블을 지정해줘야 함..\n",
    "# y_train0 = [y for _, y in image_datasets['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [13590 12503 13018 ... 14742 10255  4869] val: [ 1847  4913  9203 ...  4581 12573 12464]\n",
      "11806 2952\n"
     ]
    }
   ],
   "source": [
    "# for train_index, val_index in sss.split(indices, y_train0):\n",
    "#     print('train:', train_index, 'val:', val_index)\n",
    "#     print(len(train_index), len(val_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11806 2952 3694\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import Subset\n",
    "\n",
    "# train_ds = Subset(image_datasets['train'], train_index)\n",
    "# val_ds = Subset(image_datasets['train'], val_index)\n",
    "\n",
    "# train_size = len(train_ds)\n",
    "# val_size = len(val_ds)\n",
    "# test_size = len(image_datasets['test'])\n",
    "\n",
    "# print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({11: 3802, 9: 778, 10: 759, 8: 742, 5: 737, 1: 735, 0: 730, 4: 722, 3: 712, 7: 700, 2: 695, 6: 694})\n",
      "Counter({11: 950, 9: 195, 10: 190, 8: 185, 5: 184, 1: 184, 0: 182, 4: 181, 3: 178, 7: 175, 2: 174, 6: 174})\n"
     ]
    }
   ],
   "source": [
    "# # val_ds, test_ds의 클래스별 이미지수 확인\n",
    "# # count the number of images per class in val_ds and test_ds \n",
    "# import collections\n",
    "# import numpy as np\n",
    "\n",
    "# y_train = [y for _, y in train_ds]\n",
    "# y_val = [y for _, y in val_ds]\n",
    "\n",
    "# counter_train = collections.Counter(y_train)\n",
    "# counter_val = collections.Counter(y_val)\n",
    "# print(counter_train)\n",
    "# print(counter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = torch.utils.data.DataLoader(train_ds , batch_size=16,shuffle=True, num_workers=2)\n",
    "# valloader = torch.utils.data.DataLoader(val_ds, batch_size=16,shuffle=True, num_workers=2)\n",
    "# testloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=16,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    # model = model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        # print(f\"time per 1 epoch: {time.time() - since:.2f}\")\n",
    "\n",
    "        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                # model.load_state_dict(model_state_dict)\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                # model.load_state_dict(model_state_dict)\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 데이터를 반복\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 매개변수 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "            writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
    "            # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n",
    "\n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                print(\"Update best acc! :\", best_acc)\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    # wandb.log({'test_acc': best_acc})\n",
    "    \n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# # from sklearn.metrics import accuracy_score\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # writer = SummaryWriter()\n",
    "\n",
    "# def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "#     model = model.to(device)\n",
    "#     since = time.time()\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "#         print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "#         print('-' * 10)\n",
    "#         # print(f\"time per 1 epoch: {time.time() - since:.2f}\")\n",
    "\n",
    "#         # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n",
    "#         # for phase in ['train', 'test']:\n",
    "#         #     if phase == 'train':\n",
    "#               # pretrained 모델의 학습된 파라미터를 불러옴\n",
    "#                 # model.load_state_dict(model_state_dict)\n",
    "#         model.train()  # 모델을 학습 모드로 설정\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         # 데이터를 반복\n",
    "#         for inputs, labels in trainloader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             # 매개변수 경사도를 0으로 설정\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # 순전파\n",
    "#             # 학습 시에만 연산 기록을 추적\n",
    "#             with torch.set_grad_enabled(True):\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 train_loss = criterion(outputs, labels)\n",
    "\n",
    "#             # 학습 단계인 경우 역전파 + 최적화\n",
    "#             train_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # 통계\n",
    "#             running_loss += train_loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             scheduler.step()\n",
    "\n",
    "#         epoch_train_loss = running_loss / train_size\n",
    "#         epoch_train_acc = running_corrects.double() / train_size\n",
    "\n",
    "#         print(f'train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
    "#         # writer.add_scalar(\"Loss/train\", epoch_train_loss, epoch)\n",
    "#         # writer.add_scalar(\"Acc/train\", epoch_train_acc, epoch)\n",
    "#         # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n",
    "\n",
    "#         for inputs, labels in valloader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             # 매개변수 경사도를 0으로 설정\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # 순전파\n",
    "#             # 학습 시에만 연산 기록을 추적\n",
    "#             # with torch.set_grad_enabled(mode=True):\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             val_loss = criterion(outputs, labels)\n",
    "\n",
    "#             # 학습 단계인 경우 역전파 + 최적화\n",
    "#             val_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # 통계\n",
    "#             running_loss += val_loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             scheduler.step()\n",
    "\n",
    "#         epoch_val_loss = running_loss / val_size\n",
    "#         epoch_val_acc = running_corrects.double() / val_size\n",
    "\n",
    "#         print(f'val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n",
    "#         # writer.add_scalar(\"Loss/val\", epoch_val_loss, epoch)\n",
    "#         # writer.add_scalar(\"Acc/val\", epoch_val_acc, epoch)\n",
    "#         # wandb.log({'train_acc': epoch_acc, 'train_loss' : epoch_loss})\n",
    "\n",
    "#         if epoch_train_acc > best_acc:\n",
    "#             best_acc = epoch_train_acc\n",
    "#             print(f'update best acc: {best_acc:4f}')\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    \n",
    "            \n",
    "#             # else:\n",
    "#             #     # 코드 출처: https://blog.nerdfactory.ai/2020/11/04/Tutorial-2-for-Deep-Learning-Model-Trainer-Development.html\n",
    "#             #     # model.load_state_dict(torch.load(PATH))\n",
    "#             #     model.eval()   # 모델을 평가 모드로 설정\n",
    "#             #     sum_eval_acc, sum_eval_loss = 0, 0\n",
    "#             #     eval_result = {\"mean_loss\": 0, \"mean_acc\": 0}\n",
    "\n",
    "#             #     eval_iterator = tqdm(  # Description 양식 지정\n",
    "#             #         testloader, desc='Evaluating - mean_loss: XXX, mean_acc: XXX'\n",
    "#             #     )\n",
    "\n",
    "#             #     # Evaluate\n",
    "#             #     for e_step, e_batch in enumerate(eval_iterator):\n",
    "#             #         image_tensor, tags = map(lambda elm: elm.to(device), e_batch)  # device 에 연산용 텐서 할당\n",
    "#             #         out = model(image_tensor)  # Calculate\n",
    "#             #         loss = criterion(out, tags)\n",
    "\n",
    "#             #         # Calculate acc & loss\n",
    "#             #         sum_eval_acc += (out.max(dim=1)[1] == tags).float().mean().item()  # 정답과 추론 값이 일치하는 경우 정답으로 count\n",
    "#             #         sum_eval_loss += loss.item()\n",
    "\n",
    "#             #         writer.add_scalar(\"Loss/test\", sum_eval_loss, epoch)\n",
    "#             #         writer.add_scalar(\"Acc/test\", sum_eval_acc, epoch)\n",
    "\n",
    "#             #         eval_result.update({\"mean_loss\": sum_eval_acc / (e_step + 1),\n",
    "#             #                 \"mean_acc\": sum_eval_loss / (e_step + 1)})\n",
    "\n",
    "#             #       # Step Description\n",
    "#             #         eval_iterator.set_description(\n",
    "#             #             'Evaluating - mean_loss: {:.3f}, mean_acc: {:.3f}'.format(\n",
    "#             #                 eval_result['mean_loss'], eval_result['mean_acc'])\n",
    "#             #         )\n",
    "   \n",
    "#         print()\n",
    "#         time.sleep(0.1)\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "#     print(f'Best Acc: {best_acc:4f}')\n",
    "#     # print('Test result :', eval_result)\n",
    "#     # wandb.log({'test_acc': best_acc})\n",
    "    \n",
    "\n",
    "#     # 가장 나은 모델 가중치를 불러옴\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model\n",
    "\n",
    "# def evaluate(model, criterion, eval_dataloader):\n",
    "#     # Evaluation\n",
    "#     model.eval()  # 모델의 AutoGradient 연산을 비활성화하고 평가 연산 모드로 설정 (메모리 사용 및 연산 효율화를 위해)\n",
    "#     sum_eval_acc, sum_eval_loss = 0, 0\n",
    "#     eval_result = {\"mean_loss\": 0, \"mean_acc\": 0}\n",
    "\n",
    "#     eval_iterator = tqdm(  # Description 양식 지정\n",
    "#         eval_dataloader, desc='Evaluating - mean_loss: XXX, mean_acc: XXX'\n",
    "#     )\n",
    "\n",
    "#     # Evaluate\n",
    "#     for e_step, e_batch in enumerate(eval_iterator):\n",
    "#         image_tensor, tags = map(lambda elm: elm.to(device), e_batch)  # device 에 연산용 텐서 할당\n",
    "#         out = model(image_tensor)  # Calculate\n",
    "#         loss = criterion(out, tags)\n",
    "\n",
    "#         # Calculate acc & loss\n",
    "#         sum_eval_acc += (out.max(dim=1)[1] == tags).float().mean().item()  # 정답과 추론 값이 일치하는 경우 정답으로 count\n",
    "#         sum_eval_loss += loss.item()\n",
    "\n",
    "#         # writer.add_scalar(\"Loss/test\", sum_eval_loss, e_step)\n",
    "#         # writer.add_scalar(\"Acc/test\", sum_eval_acc, e_step)\n",
    "#         # 평가 결과 업데이트\n",
    "#         eval_result.update({\"mean_loss\": sum_eval_acc / (e_step + 1),\n",
    "#                             \"mean_acc\": sum_eval_loss / (e_step + 1)})\n",
    "\n",
    "#         # Step Description\n",
    "#         eval_iterator.set_description(\n",
    "#             'Evaluating - mean_loss: {:.3f}, mean_acc: {:.3f}'.format(\n",
    "#                 eval_result['mean_loss'], eval_result['mean_acc'])\n",
    "#         )\n",
    "#     print(f'test Loss: {sum_eval_loss:.4f} Acc: {sum_eval_acc:.4f}')\n",
    "\n",
    "#     # model.train()  # 평가 과정이 모두 종료 된 뒤, 다시 모델을 train 모드로 변경\n",
    "\n",
    "#     return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.5072 Acc: 0.8397\n",
      "val Loss: 0.9766 Acc: 0.7139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [09:43<3:04:53, 583.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.3867 Acc: 0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [19:30<2:55:41, 585.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0124 Acc: 0.6903\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.3188 Acc: 0.8959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [29:18<2:46:09, 586.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8966 Acc: 0.7715\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.2699 Acc: 0.9133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [39:05<2:36:31, 586.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7107 Acc: 0.8070\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.9244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [48:53<2:26:48, 587.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7757 Acc: 0.7639\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.2196 Acc: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [58:42<2:17:09, 587.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8482 Acc: 0.7737\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.2022 Acc: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [1:08:30<2:07:22, 587.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8120 Acc: 0.7856\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.1870 Acc: 0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [1:18:17<1:57:30, 587.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8632 Acc: 0.7937\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.9431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [1:28:05<1:47:43, 587.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8236 Acc: 0.7639\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViG-Ti\n",
    "model_ti = pvig_ti_224()\n",
    "model_ti = model_ti.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViG-S\n",
    "model_ti = pvig_s_224()\n",
    "model_ti = model_ti.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViG-M\n",
    "model_ti = pvig_m_224()\n",
    "model_ti = model_ti.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.95)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained CNN  \n",
    "### Compared to resnet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 resnet 모델 불러오기\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# 마지막 연결계층의 입력부분까지 받아오기 > 완전계층 이후 부분은 학습\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# 여기서 각 출력 샘플의 크기는 2로 설정합니다.\n",
    "# 또는, nn.Linear(num_ftrs, len (class_names))로 일반화할 수 있습니다.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 12)\n",
    "model_ft.fc = model_ft.fc.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모든 매개변수들이 최적화되었는지 관찰\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 7 에폭마다 0.1씩 학습률 감소\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8616a860257f6741c1b5f4a77ff6ccf51a19eff03d81799217c8f280496915f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
